{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_28896\\4273346668.py:22: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  integral, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_28896\\4273346668.py:37: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  denominator, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_28896\\4273346668.py:42: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  numerator, _ = quad(lambda e: np.interp(e, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global breakpoints calculation complete. Results saved to ./all_hours_optimal_segments_breakpoints.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Define a function to concatenate multiple columns into a single column\n",
    "def concatenate_columns(df):\n",
    "    return df.values.flatten()\n",
    "\n",
    "# Compute the 2/5 power integral of the second derivative after Savitzky-Golay filtering\n",
    "def compute_second_derivative_integral(x, y):\n",
    "    if np.any(np.diff(x) <= 0):\n",
    "        print(\"Warning: x is not strictly increasing, skipping second derivative computation\")\n",
    "        return np.nan\n",
    "    # Compute second derivative using Savitzky-Golay filter\n",
    "    y_savgol_second_derivative = savgol_filter(y, window_length=min(31, len(y) - 1), polyorder=3, deriv=2)\n",
    "    # Compute the integral of the 2/5 power\n",
    "    integral, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
    "    return integral ** (5/2)\n",
    "\n",
    "# Compute the optimal number of segments K\n",
    "def compute_optimal_segments(x, y, tau=1e-7):\n",
    "    integral_value = compute_second_derivative_integral(x, y)\n",
    "    if np.isnan(integral_value):\n",
    "        return np.nan\n",
    "    K = np.sqrt(integral_value / (np.sqrt(120) * tau))\n",
    "    return int(np.ceil(K))\n",
    "\n",
    "# Compute the cumulative distribution function F(e)\n",
    "def compute_cumulative_distribution(x, y):\n",
    "    y_savgol_second_derivative = savgol_filter(y, window_length=min(31, len(y) - 1), polyorder=3, deriv=2)\n",
    "    # Compute the denominator integral\n",
    "    denominator, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
    "    \n",
    "    # Compute the cumulative distribution F(ε)\n",
    "    F_epsilon = np.zeros_like(x)\n",
    "    for i, eps in enumerate(x):\n",
    "        numerator, _ = quad(lambda e: np.interp(e, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), eps)\n",
    "        F_epsilon[i] = numerator / denominator\n",
    "    \n",
    "    return F_epsilon\n",
    "\n",
    "# Compute breakpoints\n",
    "def compute_breakpoints(x, y, K):\n",
    "    if np.isnan(K) or K <= 1:\n",
    "        return []\n",
    "    F_epsilon = compute_cumulative_distribution(x, y)\n",
    "    # Construct interpolation function for F(e)\n",
    "    F_interp = interp1d(F_epsilon, x, kind=\"linear\", bounds_error=False, fill_value=(x.min(), x.max()))\n",
    "    # Compute uniformly divided breakpoints\n",
    "    breakpoints = F_interp(np.linspace(0, 1, K + 1)[1:-1])  # Remove 0 and 1\n",
    "    # Insert 0 into correct position to ensure strictly increasing breakpoints\n",
    "    breakpoints = np.sort(np.append(breakpoints, 0))\n",
    "    return breakpoints\n",
    "\n",
    "# Process data from all 24 hours\n",
    "def process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    # Read and merge data from all hours\n",
    "    for hour in range(1, 25):\n",
    "        # Load load_variation file, read only the first column\n",
    "        load_variation_file = os.path.join(load_variation_dir, f'load_variation_divided_{hour}.csv')\n",
    "        if not os.path.exists(load_variation_file):\n",
    "            print(f\"File {load_variation_file} does not exist, skipping.\")\n",
    "            continue\n",
    "        load_variation_data = pd.read_csv(load_variation_file, usecols=[0])\n",
    "        x = np.array(concatenate_columns(load_variation_data))\n",
    "        \n",
    "        # Load cost_differences file, read only the first column\n",
    "        cost_differences_file = os.path.join(hour_dir, str(hour), 'divided_costs.csv')\n",
    "        if not os.path.exists(cost_differences_file):\n",
    "            print(f\"File {cost_differences_file} does not exist, skipping.\")\n",
    "            continue\n",
    "        cost_differences_data = pd.read_csv(cost_differences_file, usecols=[0])\n",
    "        y = np.array(concatenate_columns(cost_differences_data))\n",
    "        \n",
    "        # Merge data\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "    \n",
    "    # Sort x and y and remove duplicates\n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    sort_indices = np.argsort(all_x)\n",
    "    x_sorted = all_x[sort_indices]\n",
    "    y_sorted = all_y[sort_indices]\n",
    "    x_sorted, unique_indices = np.unique(x_sorted, return_index=True)\n",
    "    y_sorted = y_sorted[unique_indices]\n",
    "    \n",
    "    # Compute smoothed curve using Savitzky-Golay filter\n",
    "    y_savgol_smooth = savgol_filter(y_sorted, window_length=min(31, len(y_sorted) - 1), polyorder=3)\n",
    "    \n",
    "    # Compute optimal number of segments K\n",
    "    if len(x_sorted) > 3:\n",
    "        K_opt = compute_optimal_segments(x_sorted, y_savgol_smooth, tau=1e-7)\n",
    "    else:\n",
    "        K_opt = np.nan\n",
    "    \n",
    "    # Compute breakpoint positions\n",
    "    breakpoints = compute_breakpoints(x_sorted, y_savgol_smooth, K_opt)\n",
    "    \n",
    "    # # Plot cumulative distribution function F(ε)\n",
    "    # F_epsilon = compute_cumulative_distribution(x_sorted, y_savgol_smooth)\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(x_sorted, F_epsilon, color='blue', label='$F(\\\\epsilon)$')\n",
    "    # for bp in breakpoints:\n",
    "    #     plt.axvline(x=bp, color='green', linestyle='--')  # Draw breakpoints\n",
    "    # plt.title('Cumulative Distribution Function (All Hours)')\n",
    "    # plt.xlabel('$\\\\epsilon$')\n",
    "    # plt.ylabel('$F(\\\\epsilon)$')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig(os.path.join(output_dir, 'all_hours_cumulative_distribution.png'))\n",
    "    # plt.close()\n",
    "    \n",
    "    # # Plot smoothed curve and mark breakpoints\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(x_sorted, y_savgol_smooth, color='red', label='Smoothing spline')\n",
    "    # plt.scatter(breakpoints, np.interp(breakpoints, x_sorted, y_savgol_smooth), color='black', marker='D', label='Breakpoints')\n",
    "    # plt.title('Smoothing Spline and Breakpoints (All Hours)')\n",
    "    # plt.xlabel('$\\\\epsilon$')\n",
    "    # plt.ylabel('$S(\\\\epsilon)$')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig(os.path.join(output_dir, 'all_hours_smooth_spline_with_breakpoints.png'))\n",
    "    # plt.close()\n",
    "    \n",
    "    # Save global breakpoints and Optimal_Segments_K\n",
    "    results_df = pd.DataFrame({'Optimal_Segments_K': [K_opt], 'Breakpoints': [breakpoints]})\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Global breakpoints calculation complete. Results saved to {output_csv}\")\n",
    "\n",
    "# Define paths\n",
    "load_variation_dir = './load_variation'\n",
    "hour_dir = './hour'\n",
    "output_dir = './breakpoints_plots'\n",
    "output_csv = './all_hours_optimal_segments_breakpoints.csv'\n",
    "\n",
    "# Run the process\n",
    "process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ./global_piecewise_linear_plots_continuous\\global_piecewise_fit_continuous.png\n",
      "Global piecewise linear fitting results saved to ./global_piecewise_linear_fit_results_continuous.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\function_base.py:151: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\85033\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\function_base.py:161: RuntimeWarning: invalid value encountered in add\n",
      "  y += start\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Linear fitting function\n",
    "def fit_linear_segment(x, y):\n",
    "    \"\"\" Perform linear fitting on a single segment, return slope and intercept \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), y)\n",
    "    return model.coef_[0], model.intercept_\n",
    "\n",
    "# Constrained linear fitting function\n",
    "def fit_constrained_segment(x, y, split_point, y_constraint):\n",
    "    \"\"\" Perform constrained linear fitting on a single segment, forcing the line to pass through (split_point, y_constraint) \"\"\"\n",
    "    x_centered = x - split_point\n",
    "    slope = np.sum(x_centered * (y - y_constraint)) / np.sum(x_centered ** 2)\n",
    "    intercept = y_constraint - slope * split_point\n",
    "    return slope, intercept\n",
    "\n",
    "# Process data from all hours and perform global piecewise linear fitting\n",
    "def process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv):\n",
    "    \"\"\" Read data -> Merge datasets -> Perform global piecewise linear fitting -> Save plots and fitting parameters \"\"\"\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    \n",
    "    # Read and merge data from all hours\n",
    "    for hour in range(1, 25):\n",
    "        # Load load_variation file, read only the first column\n",
    "        load_variation_file = os.path.join(load_variation_dir, f'load_variation_divided_{hour}.csv')\n",
    "        if not os.path.exists(load_variation_file):\n",
    "            print(f\"File {load_variation_file} does not exist, skipping Hour {hour}.\")\n",
    "            continue\n",
    "        \n",
    "        # Load cost_differences file, read only the first column\n",
    "        cost_differences_file = os.path.join(hour_dir, str(hour), 'divided_costs.csv')\n",
    "        if not os.path.exists(cost_differences_file):\n",
    "            print(f\"File {cost_differences_file} does not exist, skipping Hour {hour}.\")\n",
    "            continue\n",
    "        \n",
    "        # Read only the first column\n",
    "        load_variation_data = pd.read_csv(load_variation_file, usecols=[0])\n",
    "        cost_differences_data = pd.read_csv(cost_differences_file, usecols=[0])\n",
    "        \n",
    "        # Merge data\n",
    "        all_x.extend(load_variation_data.values.flatten())\n",
    "        all_y.extend(cost_differences_data.values.flatten())\n",
    "    \n",
    "    # Convert to NumPy arrays and sort\n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    sort_indices = np.argsort(all_x)\n",
    "    x_sorted = all_x[sort_indices]\n",
    "    y_sorted = all_y[sort_indices]\n",
    "    \n",
    "    # Read global breakpoints\n",
    "    breakpoints_df = pd.read_csv('./all_hours_optimal_segments_breakpoints.csv')\n",
    "    if breakpoints_df.empty or 'Breakpoints' not in breakpoints_df.columns:\n",
    "        print(\"Global breakpoints not found, skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Parse breakpoints\n",
    "    breakpoints_str = breakpoints_df['Breakpoints'].values[0]\n",
    "    try:\n",
    "        if isinstance(breakpoints_str, str):\n",
    "            breakpoints_str = breakpoints_str.strip('[]').replace('\\n', ' ').strip()\n",
    "            breakpoints = list(map(float, breakpoints_str.split()))\n",
    "        else:\n",
    "            breakpoints = list(breakpoints_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse Breakpoints: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Add boundaries and ensure first breakpoint is -inf and last is inf\n",
    "    breakpoints = [-np.inf] + breakpoints + [np.inf]\n",
    "    \n",
    "    # Store fitting parameters\n",
    "    segment_results = []\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_sorted, y_sorted, color='blue', label='Original Data', alpha=0.5)\n",
    "    \n",
    "    # Iterate through segments and fit (ensure segments are continuous)\n",
    "    prev_y = None\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        x_segment = x_sorted[(x_sorted > breakpoints[i]) & (x_sorted <= breakpoints[i + 1])]\n",
    "        y_segment = y_sorted[(x_sorted > breakpoints[i]) & (x_sorted <= breakpoints[i + 1])]\n",
    "        if len(x_segment) > 1:  # Need at least 2 points to fit\n",
    "            if prev_y is not None:\n",
    "                slope, intercept = fit_constrained_segment(x_segment, y_segment, breakpoints[i], prev_y)\n",
    "            else:\n",
    "                slope, intercept = fit_linear_segment(x_segment, y_segment)\n",
    "            # Store fitting results\n",
    "            segment_results.append([i + 1, breakpoints[i], breakpoints[i + 1], slope, intercept])\n",
    "            # Plot segment line\n",
    "            x_fit = np.linspace(breakpoints[i], breakpoints[i + 1], 100)\n",
    "            y_fit = slope * x_fit + intercept\n",
    "            plt.plot(x_fit, y_fit, label=f'Segment {i+1}', linewidth=2)\n",
    "            # Update prev_y for continuity\n",
    "            prev_y = slope * breakpoints[i + 1] + intercept\n",
    "        else:\n",
    "            print(f\"Insufficient points at breakpoint {breakpoints[i + 1]}, skipping segment.\")\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.title('Global Piecewise Linear Fit (Continuous)')\n",
    "    plt.xlabel('Load Deviation')\n",
    "    plt.ylabel('Cost Deviation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_file = os.path.join(output_dir, 'global_piecewise_fit_continuous.png')\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {plot_file}\")\n",
    "    \n",
    "    # Ensure first breakpoint is -inf and last is inf\n",
    "    if segment_results[0][1] != -np.inf:\n",
    "        segment_results[0][1] = -np.inf\n",
    "    if segment_results[-1][2] != np.inf:\n",
    "        segment_results[-1][2] = np.inf\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(segment_results, columns=['Segment', 'Breakpoint_Start', 'Breakpoint_End', 'Slope', 'Intercept'])\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Global piecewise linear fitting results saved to {output_csv}\")\n",
    "\n",
    "# Define paths\n",
    "load_variation_dir = './load_variation'\n",
    "hour_dir = './hour'\n",
    "output_dir = './global_piecewise_linear_plots_continuous'\n",
    "output_csv = './global_piecewise_linear_fit_results_continuous.csv'\n",
    "\n",
    "# Run global piecewise linear fitting\n",
    "process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing results have been saved to smooth_breakpoints_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def smooth_segment(breakpoint, slope_left, intercept_left, slope_right, intercept_right, delta=0.0001):\n",
    "    \"\"\"\n",
    "    Perform smoothing at the breakpoint using a quadratic curve approximation.\n",
    "    :param breakpoint: The location of the breakpoint\n",
    "    :param slope_left: Slope of the linear segment on the left\n",
    "    :param intercept_left: Intercept of the linear segment on the left\n",
    "    :param slope_right: Slope of the linear segment on the right\n",
    "    :param intercept_right: Intercept of the linear segment on the right\n",
    "    :param delta: Range around the breakpoint to apply smoothing\n",
    "    :return: Coefficients a, b, c of the quadratic curve\n",
    "    \"\"\"\n",
    "    # Positions on the left and right of the breakpoint\n",
    "    x_left = breakpoint - delta\n",
    "    x_right = breakpoint + delta\n",
    "    \n",
    "    # Value and derivative of the left linear segment at x_left\n",
    "    y_left = slope_left * x_left + intercept_left\n",
    "    dy_left = slope_left\n",
    "    \n",
    "    # Value and derivative of the right linear segment at x_right\n",
    "    y_right = slope_right * x_right + intercept_right\n",
    "    dy_right = slope_right\n",
    "    \n",
    "    # Construct equations to solve for quadratic coefficients\n",
    "    A = np.array([[x_left**2, x_left, 1],\n",
    "                  [x_right**2, x_right, 1],\n",
    "                  [2 * x_left, 1, 0],\n",
    "                  [2 * x_right, 1, 0]])\n",
    "    b = np.array([y_left, y_right, dy_left, dy_right])\n",
    "    \n",
    "    # Solve for quadratic coefficients\n",
    "    a, b, c = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    \n",
    "    # Validate that the curve meets the boundary conditions\n",
    "    y_left_curve = a * x_left**2 + b * x_left + c\n",
    "    y_right_curve = a * x_right**2 + b * x_right + c\n",
    "    dy_left_curve = 2 * a * x_left + b\n",
    "    dy_right_curve = 2 * a * x_right + b\n",
    "    \n",
    "    assert np.isclose(y_left_curve, y_left), \"Left value condition not satisfied\"\n",
    "    assert np.isclose(y_right_curve, y_right), \"Right value condition not satisfied\"\n",
    "    assert np.isclose(dy_left_curve, dy_left), \"Left derivative condition not satisfied\"\n",
    "    assert np.isclose(dy_right_curve, dy_right), \"Right derivative condition not satisfied\"\n",
    "    \n",
    "    return a, b, c\n",
    "\n",
    "def smooth_breakpoints(results_df, delta=0.0001):\n",
    "    \"\"\"\n",
    "    Apply smoothing to all breakpoints and store the results in a new CSV file.\n",
    "    :param results_df: Original piecewise linear fitting results\n",
    "    :param delta: Range around the breakpoint to apply smoothing\n",
    "    \"\"\"\n",
    "    smoothed_results = []\n",
    "    \n",
    "    for i in range(len(results_df) - 1):\n",
    "        segment_left = results_df.iloc[i]\n",
    "        segment_right = results_df.iloc[i + 1]\n",
    "        \n",
    "        # Breakpoint position\n",
    "        breakpoint_left = segment_left['Breakpoint_End']\n",
    "        breakpoint_right = segment_right['Breakpoint_Start']\n",
    "        \n",
    "        # If breakpoints match, apply smoothing\n",
    "        if breakpoint_left == breakpoint_right and not np.isinf(breakpoint_left):\n",
    "            # Get slopes and intercepts from both sides\n",
    "            slope_left = segment_left['Slope']\n",
    "            intercept_left = segment_left['Intercept']\n",
    "            slope_right = segment_right['Slope']\n",
    "            intercept_right = segment_right['Intercept']\n",
    "            \n",
    "            # Perform smoothing\n",
    "            a, b, c = smooth_segment(breakpoint_left, slope_left, intercept_left, slope_right, intercept_right, delta)\n",
    "            \n",
    "            # Store smoothed result\n",
    "            smoothed_results.append([breakpoint_left, a, b, c])\n",
    "    \n",
    "    # Save smoothed results to a new CSV file\n",
    "    smoothed_df = pd.DataFrame(smoothed_results, columns=['Breakpoint', 'A', 'B', 'C'])\n",
    "    smoothed_df.to_csv('./smooth_breakpoints_results.csv', index=False)\n",
    "    print(\"Smoothing results have been saved to smooth_breakpoints_results.csv\")\n",
    "\n",
    "# Read original piecewise linear fitting results\n",
    "results_df = pd.read_csv('./global_piecewise_linear_fit_results_continuous.csv')\n",
    "\n",
    "# Perform smoothing\n",
    "smooth_breakpoints(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Training MSE=92908.1724\n",
      "Iteration 200: Training MSE=92873.9988\n",
      "Iteration 300: Training MSE=92873.9988\n",
      "Iteration 400: Training MSE=92873.9988\n",
      "Iteration 500: Training MSE=92873.9988\n",
      "Iteration 600: Training MSE=92873.9988\n",
      "Iteration 700: Training MSE=92873.9988\n",
      "Iteration 800: Training MSE=92873.9988\n",
      "Iteration 900: Training MSE=92873.9988\n",
      "Iteration 1000: Training MSE=92873.9988\n",
      "Final Training Set: MSE=92873.9988, RMSE=304.7524, MAE=247.7969\n",
      "Final Test Set: MSE=135356.5933, RMSE=367.9084, MAE=302.9241\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "fit_results_path = './global_piecewise_linear_fit_results_continuous.csv'  # Piecewise linear fitting results\n",
    "smooth_results_path = './smooth_breakpoints_results.csv'  # Smoothing results\n",
    "output_dir = './predictions'  # Output folder for prediction results\n",
    "model_params_dir = './model_params'  # Folder to save model parameters\n",
    "epsilon_dir = './epsilon_values'  # Folder to save epsilon values during iterations\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_params_dir, exist_ok=True)\n",
    "os.makedirs(epsilon_dir, exist_ok=True)\n",
    "\n",
    "# Read fitting and smoothing results\n",
    "fit_results_df = pd.read_csv(fit_results_path)\n",
    "smooth_results_df = pd.read_csv(smooth_results_path)\n",
    "\n",
    "# Load training and testing data\n",
    "X_train = pd.read_csv('./X_train.csv').values\n",
    "y_train = pd.read_csv('./Y_train.csv').values.reshape(-1, 1)\n",
    "X_test = pd.read_csv('./X_test.csv').values\n",
    "y_test = pd.read_csv('./Y_test.csv').values.reshape(-1, 1)\n",
    "\n",
    "# Apply Min-Max normalization to X in range [-1, 1]\n",
    "X_train_min = np.min(X_train, axis=0)\n",
    "X_train_max = np.max(X_train, axis=0)\n",
    "X_train = 2 * (X_train - X_train_min) / (X_train_max - X_train_min) - 1\n",
    "X_test = 2 * (X_test - X_train_min) / (X_train_max - X_train_min) - 1\n",
    "\n",
    "# Add a bias term (all ones) as the last column of X_train and X_test\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "# Hyperparameters\n",
    "delta = 0.0001  # Piecewise threshold\n",
    "gamma = 0.001  # Learning rate decay\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "eps = 0  # To avoid division by zero\n",
    "\n",
    "\n",
    "# Compute piecewise gradient\n",
    "def piecewise_gradient(epsilon_i, delta, a_k, breakpoints, smooth_results_df):\n",
    "    \"\"\" Compute gradient for sample i \"\"\"\n",
    "    grad = 0\n",
    "    segment_index = np.digitize([epsilon_i], breakpoints)[0] - 1  # Determine which segment the point falls into\n",
    "    segment_index = int(np.clip(segment_index, 0, len(a_k) - 1))  # Ensure index is within bounds\n",
    "\n",
    "    # Check if point is near the breakpoint\n",
    "    if np.abs(epsilon_i - breakpoints[segment_index]) < delta:\n",
    "        # Query smoothing parameters for the quadratic function\n",
    "        smooth_params = smooth_results_df[smooth_results_df['Breakpoint'] == breakpoints[segment_index]]\n",
    "        if not smooth_params.empty:\n",
    "            a_quad = smooth_params['A'].values[0]\n",
    "            b_quad = smooth_params['B'].values[0]\n",
    "            # Compute gradient from quadratic function\n",
    "            grad = 2 * a_quad * epsilon_i + b_quad\n",
    "    else:\n",
    "        # Within a normal linear segment, use slope a_k[segment_index]\n",
    "        grad = a_k[segment_index]\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mse, rmse, mae\n",
    "\n",
    "\n",
    "# Training and prediction function\n",
    "def train_and_predict():\n",
    "    \"\"\" Train and predict on the entire dataset \"\"\"\n",
    "    a_k = fit_results_df['Slope'].values  # Slopes a_k\n",
    "    breakpoints = fit_results_df['Breakpoint_Start'].values  # Breakpoints\n",
    "\n",
    "    # Initialize parameters w\n",
    "    n, d = X_train.shape\n",
    "    w = np.zeros((d, 1))\n",
    "    eta = 6  # Initial learning rate\n",
    "\n",
    "    # Training loop\n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad_w = np.zeros_like(w)\n",
    "        for i in range(n):  # Loop over all samples\n",
    "            epsilon_i = (X_train[i] @ w - y_train[i]) / y_train[i]\n",
    "            grad_L_i = piecewise_gradient(epsilon_i, delta, a_k, breakpoints, smooth_results_df)\n",
    "            grad_w += (grad_L_i / y_train[i]) * X_train[i].reshape(-1, 1)\n",
    "        w -= eta * grad_w  # Gradient update\n",
    "        eta = eta / (1 + gamma * t)  # Learning rate decay\n",
    "\n",
    "        # Print progress every 100 iterations\n",
    "        if t % 100 == 0:\n",
    "            y_pred_train = X_train @ w\n",
    "            mse_train, _, _ = evaluate(y_train, y_pred_train)\n",
    "            print(f\"Iteration {t}: Training MSE={mse_train:.4f}\")\n",
    "            np.save(os.path.join(model_params_dir, f'w_iter_{t}.npy'), w)\n",
    "\n",
    "    # Predictions on training and test sets\n",
    "    y_pred_train = X_train @ w\n",
    "    y_pred_test = X_test @ w\n",
    "\n",
    "    # Compute final evaluation metrics\n",
    "    mse_train, rmse_train, mae_train = evaluate(y_train, y_pred_train)\n",
    "    mse_test, rmse_test, mae_test = evaluate(y_test, y_pred_test)\n",
    "    print(f\"Final Training Set: MSE={mse_train:.4f}, RMSE={rmse_train:.4f}, MAE={mae_train:.4f}\")\n",
    "    print(f\"Final Test Set: MSE={mse_test:.4f}, RMSE={rmse_test:.4f}, MAE={mae_test:.4f}\")\n",
    "\n",
    "    # Save prediction results\n",
    "    pd.DataFrame({\"y_true\": y_train.flatten(), \"y_pred\": y_pred_train.flatten()}).to_csv(\n",
    "        os.path.join(output_dir, 'predictions_train.csv'), index=False)\n",
    "    pd.DataFrame({\"y_true\": y_test.flatten(), \"y_pred\": y_pred_test.flatten()}).to_csv(\n",
    "        os.path.join(output_dir, 'prediction_GB.csv'), index=False)\n",
    "\n",
    "    # Save final model parameters\n",
    "    np.save(os.path.join(model_params_dir, 'w_final.npy'), w)\n",
    "\n",
    "\n",
    "# Run training and prediction\n",
    "train_and_predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNLLL",
   "language": "python",
   "name": "nnlll"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
