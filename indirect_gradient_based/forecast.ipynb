{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成: ./hour\\1\n",
      "处理完成: ./hour\\2\n",
      "处理完成: ./hour\\3\n",
      "处理完成: ./hour\\4\n",
      "处理完成: ./hour\\5\n",
      "处理完成: ./hour\\6\n",
      "处理完成: ./hour\\7\n",
      "处理完成: ./hour\\8\n",
      "处理完成: ./hour\\9\n",
      "处理完成: ./hour\\10\n",
      "处理完成: ./hour\\11\n",
      "处理完成: ./hour\\12\n",
      "处理完成: ./hour\\13\n",
      "处理完成: ./hour\\14\n",
      "处理完成: ./hour\\15\n",
      "处理完成: ./hour\\16\n",
      "处理完成: ./hour\\17\n",
      "处理完成: ./hour\\18\n",
      "处理完成: ./hour\\19\n",
      "处理完成: ./hour\\20\n",
      "处理完成: ./hour\\21\n",
      "处理完成: ./hour\\22\n",
      "处理完成: ./hour\\23\n",
      "处理完成: ./hour\\24\n",
      "所有文件夹处理完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义主目录路径\n",
    "base_dir = './hour'\n",
    "\n",
    "# 遍历1到24文件夹\n",
    "for folder_num in range(1, 25):\n",
    "    folder_path = os.path.join(base_dir, str(folder_num))\n",
    "    \n",
    "    # 读取cost_differences.csv\n",
    "    cost_diff_path = os.path.join(folder_path, 'cost_differences.csv')\n",
    "    cost_diff_df = pd.read_csv(cost_diff_path)\n",
    "    \n",
    "    # 读取perfect_costs.csv\n",
    "    perfect_cost_path = os.path.join(folder_path, 'perfect_costs.csv')\n",
    "    perfect_cost_df = pd.read_csv(perfect_cost_path)\n",
    "    \n",
    "    # 将Change1-Change5列与Perfect_Cost列对应数据相除\n",
    "    result_df = cost_diff_df.div(perfect_cost_df['Perfect_Cost'], axis=0)\n",
    "    \n",
    "    # 保存结果到新的CSV文件\n",
    "    result_path = os.path.join(folder_path, 'divided_costs.csv')\n",
    "    result_df.to_csv(result_path, index=False)\n",
    "    \n",
    "    print(f\"处理完成: {folder_path}\")\n",
    "\n",
    "print(\"所有文件夹处理完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成: ./load_variation\\load_variation_1.csv -> ./load_variation\\load_variation_divided_1.csv\n",
      "处理完成: ./load_variation\\load_variation_2.csv -> ./load_variation\\load_variation_divided_2.csv\n",
      "处理完成: ./load_variation\\load_variation_3.csv -> ./load_variation\\load_variation_divided_3.csv\n",
      "处理完成: ./load_variation\\load_variation_4.csv -> ./load_variation\\load_variation_divided_4.csv\n",
      "处理完成: ./load_variation\\load_variation_5.csv -> ./load_variation\\load_variation_divided_5.csv\n",
      "处理完成: ./load_variation\\load_variation_6.csv -> ./load_variation\\load_variation_divided_6.csv\n",
      "处理完成: ./load_variation\\load_variation_7.csv -> ./load_variation\\load_variation_divided_7.csv\n",
      "处理完成: ./load_variation\\load_variation_8.csv -> ./load_variation\\load_variation_divided_8.csv\n",
      "处理完成: ./load_variation\\load_variation_9.csv -> ./load_variation\\load_variation_divided_9.csv\n",
      "处理完成: ./load_variation\\load_variation_10.csv -> ./load_variation\\load_variation_divided_10.csv\n",
      "处理完成: ./load_variation\\load_variation_11.csv -> ./load_variation\\load_variation_divided_11.csv\n",
      "处理完成: ./load_variation\\load_variation_12.csv -> ./load_variation\\load_variation_divided_12.csv\n",
      "处理完成: ./load_variation\\load_variation_13.csv -> ./load_variation\\load_variation_divided_13.csv\n",
      "处理完成: ./load_variation\\load_variation_14.csv -> ./load_variation\\load_variation_divided_14.csv\n",
      "处理完成: ./load_variation\\load_variation_15.csv -> ./load_variation\\load_variation_divided_15.csv\n",
      "处理完成: ./load_variation\\load_variation_16.csv -> ./load_variation\\load_variation_divided_16.csv\n",
      "处理完成: ./load_variation\\load_variation_17.csv -> ./load_variation\\load_variation_divided_17.csv\n",
      "处理完成: ./load_variation\\load_variation_18.csv -> ./load_variation\\load_variation_divided_18.csv\n",
      "处理完成: ./load_variation\\load_variation_19.csv -> ./load_variation\\load_variation_divided_19.csv\n",
      "处理完成: ./load_variation\\load_variation_20.csv -> ./load_variation\\load_variation_divided_20.csv\n",
      "处理完成: ./load_variation\\load_variation_21.csv -> ./load_variation\\load_variation_divided_21.csv\n",
      "处理完成: ./load_variation\\load_variation_22.csv -> ./load_variation\\load_variation_divided_22.csv\n",
      "处理完成: ./load_variation\\load_variation_23.csv -> ./load_variation\\load_variation_divided_23.csv\n",
      "处理完成: ./load_variation\\load_variation_24.csv -> ./load_variation\\load_variation_divided_24.csv\n",
      "所有文件处理完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目录路径\n",
    "load_variation_dir = './load_variation'\n",
    "hourly_data_dir = './hourly_data'\n",
    "\n",
    "# 遍历 1 到 24\n",
    "for i in range(1, 25):\n",
    "    # 构造文件路径\n",
    "    load_variation_file = os.path.join(load_variation_dir, f'load_variation_{i}.csv')\n",
    "    y_train_file = os.path.join(hourly_data_dir, f'Y_train_hour_{i}.csv')\n",
    "    \n",
    "    # 读取 load_variation 文件\n",
    "    load_variation_df = pd.read_csv(load_variation_file)\n",
    "    \n",
    "    # 读取 Y_train_hour 文件的第一列\n",
    "    y_train_df = pd.read_csv(y_train_file)\n",
    "    y_train_column = y_train_df.iloc[:, 0]  # 获取第一列数据\n",
    "    \n",
    "    # 将 load_variation 的各列与 Y_train_hour 的第一列相除\n",
    "    result_df = load_variation_df.div(y_train_column, axis=0)\n",
    "    \n",
    "    # 保存结果到新的 CSV 文件\n",
    "    result_file = os.path.join(load_variation_dir, f'load_variation_divided_{i}.csv')\n",
    "    result_df.to_csv(result_file, index=False)\n",
    "    \n",
    "    print(f\"处理完成: {load_variation_file} -> {result_file}\")\n",
    "\n",
    "print(\"所有文件处理完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\1391239269.py:22: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  integral, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\1391239269.py:37: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  denominator, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\1391239269.py:42: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  numerator, _ = quad(lambda e: np.interp(e, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局分段点计算完成，结果已保存到 ./all_hours_optimal_segments_breakpoints.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 定义函数将多列数据连接为一列\n",
    "def concatenate_columns(df):\n",
    "    return df.values.flatten()\n",
    "\n",
    "# 计算 Savitzky-Golay 滤波后的二阶导数的 2/5 次方积分\n",
    "def compute_second_derivative_integral(x, y):\n",
    "    if np.any(np.diff(x) <= 0):\n",
    "        print(\"警告: x 不是严格递增的，跳过二阶导数计算\")\n",
    "        return np.nan\n",
    "    # 计算二阶导数（Savitzky-Golay 滤波）\n",
    "    y_savgol_second_derivative = savgol_filter(y, window_length=min(31, len(y) - 1), polyorder=3, deriv=2)\n",
    "    # 计算 2/5 次方的积分\n",
    "    integral, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
    "    return integral ** (5/2)\n",
    "\n",
    "# 计算最优分段数 K\n",
    "def compute_optimal_segments(x, y, tau=1e-7):\n",
    "    integral_value = compute_second_derivative_integral(x, y)\n",
    "    if np.isnan(integral_value):\n",
    "        return np.nan\n",
    "    K = np.sqrt(integral_value / (np.sqrt(120) * tau))\n",
    "    return int(np.ceil(K))\n",
    "\n",
    "# 计算累积分布函数 F(e)\n",
    "def compute_cumulative_distribution(x, y):\n",
    "    y_savgol_second_derivative = savgol_filter(y, window_length=min(31, len(y) - 1), polyorder=3, deriv=2)\n",
    "    # 计算分母积分\n",
    "    denominator, _ = quad(lambda eps: np.interp(eps, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), np.max(x))\n",
    "    \n",
    "    # 计算累积分布 F(ε)\n",
    "    F_epsilon = np.zeros_like(x)\n",
    "    for i, eps in enumerate(x):\n",
    "        numerator, _ = quad(lambda e: np.interp(e, x, np.abs(y_savgol_second_derivative) ** (2/5)), np.min(x), eps)\n",
    "        F_epsilon[i] = numerator / denominator\n",
    "    \n",
    "    return F_epsilon\n",
    "\n",
    "# 计算分段点\n",
    "def compute_breakpoints(x, y, K):\n",
    "    if np.isnan(K) or K <= 1:\n",
    "        return []\n",
    "    F_epsilon = compute_cumulative_distribution(x, y)\n",
    "    # 构造 F(e) 的插值函数\n",
    "    F_interp = interp1d(F_epsilon, x, kind=\"linear\", bounds_error=False, fill_value=(x.min(), x.max()))\n",
    "    # 计算均匀划分的断点\n",
    "    breakpoints = F_interp(np.linspace(0, 1, K + 1)[1:-1])  # 去掉 0 和 1\n",
    "    # 将 0 插入到正确位置，确保 Breakpoints 严格递增\n",
    "    breakpoints = np.sort(np.append(breakpoints, 0))\n",
    "    return breakpoints\n",
    "\n",
    "# 处理所有小时的数据\n",
    "def process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    # 读取并合并所有小时的数据\n",
    "    for hour in range(1, 25):\n",
    "        # 加载 load_variation 文件，仅读取第一列\n",
    "        load_variation_file = os.path.join(load_variation_dir, f'load_variation_divided_{hour}.csv')\n",
    "        if not os.path.exists(load_variation_file):\n",
    "            print(f\"文件 {load_variation_file} 不存在，跳过。\")\n",
    "            continue\n",
    "        load_variation_data = pd.read_csv(load_variation_file, usecols=[0])\n",
    "        x = np.array(concatenate_columns(load_variation_data))\n",
    "        \n",
    "        # 加载 cost_differences 文件，仅读取第一列\n",
    "        cost_differences_file = os.path.join(hour_dir, str(hour), 'divided_costs.csv')\n",
    "        if not os.path.exists(cost_differences_file):\n",
    "            print(f\"文件 {cost_differences_file} 不存在，跳过。\")\n",
    "            continue\n",
    "        cost_differences_data = pd.read_csv(cost_differences_file, usecols=[0])\n",
    "        y = np.array(concatenate_columns(cost_differences_data))\n",
    "        \n",
    "        # 合并数据\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "    \n",
    "    # 对 x 和 y 进行排序，并去重\n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    sort_indices = np.argsort(all_x)\n",
    "    x_sorted = all_x[sort_indices]\n",
    "    y_sorted = all_y[sort_indices]\n",
    "    x_sorted, unique_indices = np.unique(x_sorted, return_index=True)\n",
    "    y_sorted = y_sorted[unique_indices]\n",
    "    \n",
    "    # 计算 Savitzky-Golay 滤波后的平滑曲线\n",
    "    y_savgol_smooth = savgol_filter(y_sorted, window_length=min(31, len(y_sorted) - 1), polyorder=3)\n",
    "    \n",
    "    # 计算最优分段数 K\n",
    "    if len(x_sorted) > 3:\n",
    "        K_opt = compute_optimal_segments(x_sorted, y_savgol_smooth, tau=1e-7)\n",
    "    else:\n",
    "        K_opt = np.nan\n",
    "    \n",
    "    # 计算分段点位置\n",
    "    breakpoints = compute_breakpoints(x_sorted, y_savgol_smooth, K_opt)\n",
    "    \n",
    "    # 绘制累积分布函数 F(ε)\n",
    "    F_epsilon = compute_cumulative_distribution(x_sorted, y_savgol_smooth)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_sorted, F_epsilon, color='blue', label='$F(\\\\epsilon)$')  # 修复字符串闭合问题\n",
    "    for bp in breakpoints:\n",
    "        plt.axvline(x=bp, color='green', linestyle='--')  # 画出分段点\n",
    "    plt.title('Cumulative Distribution Function (All Hours)')\n",
    "    plt.xlabel('$\\\\epsilon$')  # 修复字符串闭合问题\n",
    "    plt.ylabel('$F(\\\\epsilon)$')  # 修复字符串闭合问题\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'all_hours_cumulative_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制平滑曲线并标注断点\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_sorted, y_savgol_smooth, color='red', label='Smoothing spline')\n",
    "    plt.scatter(breakpoints, np.interp(breakpoints, x_sorted, y_savgol_smooth), color='black', marker='D', label='Breakpoints')\n",
    "    plt.title('Smoothing Spline and Breakpoints (All Hours)')\n",
    "    plt.xlabel('$\\\\epsilon$')  # 修复字符串闭合问题\n",
    "    plt.ylabel('$S(\\\\epsilon)$')  # 修复字符串闭合问题\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'all_hours_smooth_spline_with_breakpoints.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存全局的 breakpoint 和 Optimal_Segments_K\n",
    "    results_df = pd.DataFrame({'Optimal_Segments_K': [K_opt], 'Breakpoints': [breakpoints]})\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"全局分段点计算完成，结果已保存到 {output_csv}\")\n",
    "\n",
    "# 定义路径\n",
    "load_variation_dir = './load_variation'\n",
    "hour_dir = './hour'\n",
    "output_dir = './breakpoints_plots'\n",
    "output_csv = './all_hours_optimal_segments_breakpoints.csv'\n",
    "\n",
    "# 运行\n",
    "process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像已保存到 ./global_piecewise_linear_plots_continuous\\global_piecewise_fit_continuous.png\n",
      "全局分段线性拟合结果已保存到 ./global_piecewise_linear_fit_results_continuous.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\function_base.py:151: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\85033\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\function_base.py:161: RuntimeWarning: invalid value encountered in add\n",
      "  y += start\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\3094155340.py:116: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_file)\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\3094155340.py:116: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_file)\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\3094155340.py:116: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_file)\n",
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_16464\\3094155340.py:116: UserWarning: Glyph 25454 (\\N{CJK UNIFIED IDEOGRAPH-636E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_file)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 线性拟合函数\n",
    "def fit_linear_segment(x, y):\n",
    "    \"\"\" 对单个分段进行线性拟合，返回斜率和截距 \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), y)\n",
    "    return model.coef_[0], model.intercept_\n",
    "\n",
    "# 约束条件下的线性拟合函数\n",
    "def fit_constrained_segment(x, y, split_point, y_constraint):\n",
    "    \"\"\" 对单个分段进行线性拟合，约束拟合直线通过 (split_point, y_constraint) \"\"\"\n",
    "    x_centered = x - split_point\n",
    "    slope = np.sum(x_centered * (y - y_constraint)) / np.sum(x_centered ** 2)\n",
    "    intercept = y_constraint - slope * split_point\n",
    "    return slope, intercept\n",
    "\n",
    "# 处理所有小时的数据并进行全局分段线性拟合\n",
    "def process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv):\n",
    "    \"\"\" 读取数据 -> 合并数据集 -> 进行全局分段线性拟合 -> 存储图像和拟合参数 \"\"\"\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    \n",
    "    # 读取并合并所有小时的数据\n",
    "    for hour in range(1, 25):\n",
    "        # 加载 load_variation 文件，仅读取第一列\n",
    "        load_variation_file = os.path.join(load_variation_dir, f'load_variation_divided_{hour}.csv')\n",
    "        if not os.path.exists(load_variation_file):\n",
    "            print(f\"文件 {load_variation_file} 不存在，跳过 Hour {hour}。\")\n",
    "            continue\n",
    "        \n",
    "        # 加载 cost_differences 文件，仅读取第一列\n",
    "        cost_differences_file = os.path.join(hour_dir, str(hour), 'divided_costs.csv')\n",
    "        if not os.path.exists(cost_differences_file):\n",
    "            print(f\"文件 {cost_differences_file} 不存在，跳过 Hour {hour}。\")\n",
    "            continue\n",
    "        \n",
    "        # 仅读取第一列\n",
    "        load_variation_data = pd.read_csv(load_variation_file, usecols=[0])\n",
    "        cost_differences_data = pd.read_csv(cost_differences_file, usecols=[0])\n",
    "        \n",
    "        # 合并数据\n",
    "        all_x.extend(load_variation_data.values.flatten())\n",
    "        all_y.extend(cost_differences_data.values.flatten())\n",
    "    \n",
    "    # 转换为 NumPy 数组并排序\n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    sort_indices = np.argsort(all_x)\n",
    "    x_sorted = all_x[sort_indices]\n",
    "    y_sorted = all_y[sort_indices]\n",
    "    \n",
    "    # 读取全局的分段点\n",
    "    breakpoints_df = pd.read_csv('./all_hours_optimal_segments_breakpoints.csv')\n",
    "    if breakpoints_df.empty or 'Breakpoints' not in breakpoints_df.columns:\n",
    "        print(\"全局分段点不存在，跳过。\")\n",
    "        return\n",
    "    \n",
    "    # 解析分段点\n",
    "    breakpoints_str = breakpoints_df['Breakpoints'].values[0]\n",
    "    try:\n",
    "        if isinstance(breakpoints_str, str):\n",
    "            breakpoints_str = breakpoints_str.strip('[]').replace('\\n', ' ').strip()\n",
    "            breakpoints = list(map(float, breakpoints_str.split()))\n",
    "        else:\n",
    "            breakpoints = list(breakpoints_str)\n",
    "    except Exception as e:\n",
    "        print(f\"解析 Breakpoints 失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 添加边界，并确保第一个 breakpoint 为 -inf，最后一个为 inf\n",
    "    breakpoints = [-np.inf] + breakpoints + [np.inf]\n",
    "    \n",
    "    # 存储拟合参数\n",
    "    segment_results = []\n",
    "    \n",
    "    # 绘制图像\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_sorted, y_sorted, color='blue', label='原始数据', alpha=0.5)\n",
    "    \n",
    "    # 遍历分段并拟合（确保分段点连续）\n",
    "    prev_y = None\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        x_segment = x_sorted[(x_sorted > breakpoints[i]) & (x_sorted <= breakpoints[i + 1])]\n",
    "        y_segment = y_sorted[(x_sorted > breakpoints[i]) & (x_sorted <= breakpoints[i + 1])]\n",
    "        if len(x_segment) > 1:  # 至少需要 2 个点进行拟合\n",
    "            if prev_y is not None:\n",
    "                slope, intercept = fit_constrained_segment(x_segment, y_segment, breakpoints[i], prev_y)\n",
    "            else:\n",
    "                slope, intercept = fit_linear_segment(x_segment, y_segment)\n",
    "            # 存储拟合结果\n",
    "            segment_results.append([i + 1, breakpoints[i], breakpoints[i + 1], slope, intercept])\n",
    "            # 绘制分段线\n",
    "            x_fit = np.linspace(breakpoints[i], breakpoints[i + 1], 100)\n",
    "            y_fit = slope * x_fit + intercept\n",
    "            plt.plot(x_fit, y_fit, label=f'Segment {i+1}', linewidth=2)\n",
    "            # 更新 prev_y 为当前段的终点值\n",
    "            prev_y = slope * breakpoints[i + 1] + intercept\n",
    "        else:\n",
    "            print(f\"分段点 {breakpoints[i + 1]} 处点数不足，跳过该段。\")\n",
    "    \n",
    "    # 图像设置\n",
    "    plt.title('Global Piecewise Linear Fit (Continuous)')\n",
    "    plt.xlabel('Load Deviation')\n",
    "    plt.ylabel('Cost Deviation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 保存图像\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_file = os.path.join(output_dir, 'global_piecewise_fit_continuous.png')\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"图像已保存到 {plot_file}\")\n",
    "    \n",
    "    # 确保第一个 breakpoint 为 -inf，最后一个 breakpoint 为 inf\n",
    "    if segment_results[0][1] != -np.inf:\n",
    "        segment_results[0][1] = -np.inf\n",
    "    if segment_results[-1][2] != np.inf:\n",
    "        segment_results[-1][2] = np.inf\n",
    "    \n",
    "    # 保存结果到 CSV\n",
    "    results_df = pd.DataFrame(segment_results, columns=['Segment', 'Breakpoint_Start', 'Breakpoint_End', 'Slope', 'Intercept'])\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"全局分段线性拟合结果已保存到 {output_csv}\")\n",
    "\n",
    "# 定义路径\n",
    "load_variation_dir = './load_variation'\n",
    "hour_dir = './hour'\n",
    "output_dir = './global_piecewise_linear_plots_continuous'\n",
    "output_csv = './global_piecewise_linear_fit_results_continuous.csv'\n",
    "\n",
    "# 运行全局分段线性拟合\n",
    "process_all_hours(load_variation_dir, hour_dir, output_dir, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平滑化结果已保存到 smooth_breakpoints_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def smooth_segment(breakpoint, slope_left, intercept_left, slope_right, intercept_right, delta=0.0001):\n",
    "    \"\"\"\n",
    "    在分段点处进行平滑化处理，使用二次曲线近似平滑化。\n",
    "    :param breakpoint: 分段点位置\n",
    "    :param slope_left: 左侧线性曲线的斜率\n",
    "    :param intercept_left: 左侧线性曲线的截距\n",
    "    :param slope_right: 右侧线性曲线的斜率\n",
    "    :param intercept_right: 右侧线性曲线的截距\n",
    "    :param delta: 平滑化范围\n",
    "    :return: 二次曲线的系数 a, b, c\n",
    "    \"\"\"\n",
    "    # 左侧点位置和右侧点位置\n",
    "    x_left = breakpoint - delta\n",
    "    x_right = breakpoint + delta\n",
    "    \n",
    "    # 左侧线性曲线在 x_left 处的值和导数\n",
    "    y_left = slope_left * x_left + intercept_left\n",
    "    dy_left = slope_left\n",
    "    \n",
    "    # 右侧线性曲线在 x_right 处的值和导数\n",
    "    y_right = slope_right * x_right + intercept_right\n",
    "    dy_right = slope_right\n",
    "    \n",
    "    # 构建方程组求解二次曲线系数\n",
    "    A = np.array([[x_left**2, x_left, 1],\n",
    "                  [x_right**2, x_right, 1],\n",
    "                  [2 * x_left, 1, 0],\n",
    "                  [2 * x_right, 1, 0]])\n",
    "    b = np.array([y_left, y_right, dy_left, dy_right])\n",
    "    \n",
    "    # 求解二次曲线系数\n",
    "    a, b, c = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    \n",
    "    # 验证条件是否成立\n",
    "    y_left_curve = a * x_left**2 + b * x_left + c\n",
    "    y_right_curve = a * x_right**2 + b * x_right + c\n",
    "    dy_left_curve = 2 * a * x_left + b\n",
    "    dy_right_curve = 2 * a * x_right + b\n",
    "    \n",
    "    assert np.isclose(y_left_curve, y_left), \"Left value condition not satisfied\"\n",
    "    assert np.isclose(y_right_curve, y_right), \"Right value condition not satisfied\"\n",
    "    assert np.isclose(dy_left_curve, dy_left), \"Left derivative condition not satisfied\"\n",
    "    assert np.isclose(dy_right_curve, dy_right), \"Right derivative condition not satisfied\"\n",
    "    \n",
    "    return a, b, c\n",
    "\n",
    "def smooth_breakpoints(results_df, delta=0.0001):\n",
    "    \"\"\"\n",
    "    对所有分段点进行平滑化处理，并将结果存储到一个新的 CSV 文件中。\n",
    "    :param results_df: 原分段线性拟合结果\n",
    "    :param delta: 平滑化范围\n",
    "    \"\"\"\n",
    "    smoothed_results = []\n",
    "    \n",
    "    for i in range(len(results_df) - 1):\n",
    "        segment_left = results_df.iloc[i]\n",
    "        segment_right = results_df.iloc[i + 1]\n",
    "        \n",
    "        # 分段点位置\n",
    "        breakpoint_left = segment_left['Breakpoint_End']\n",
    "        breakpoint_right = segment_right['Breakpoint_Start']\n",
    "        \n",
    "        # 如果分段点相同，则进行平滑化\n",
    "        if breakpoint_left == breakpoint_right and not np.isinf(breakpoint_left):\n",
    "            # 获取左侧和右侧的斜率和截距\n",
    "            slope_left = segment_left['Slope']\n",
    "            intercept_left = segment_left['Intercept']\n",
    "            slope_right = segment_right['Slope']\n",
    "            intercept_right = segment_right['Intercept']\n",
    "            \n",
    "            # 平滑化处理\n",
    "            a, b, c = smooth_segment(breakpoint_left, slope_left, intercept_left, slope_right, intercept_right, delta)\n",
    "            \n",
    "            # 存储平滑化后的结果\n",
    "            smoothed_results.append([breakpoint_left, a, b, c])\n",
    "    \n",
    "    # 将平滑化后的结果存储到新的 CSV 文件中\n",
    "    smoothed_df = pd.DataFrame(smoothed_results, columns=['Breakpoint', 'A', 'B', 'C'])\n",
    "    smoothed_df.to_csv('./smooth_breakpoints_results.csv', index=False)\n",
    "    print(\"平滑化结果已保存到 smooth_breakpoints_results.csv\")\n",
    "\n",
    "# 读取原分段线性拟合结果\n",
    "results_df = pd.read_csv('./global_piecewise_linear_fit_results_continuous.csv')\n",
    "\n",
    "# 进行平滑化处理\n",
    "smooth_breakpoints(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 100: 训练集 MSE=71747.9491\n",
      "迭代 200: 训练集 MSE=71724.5701\n",
      "迭代 300: 训练集 MSE=71724.5701\n",
      "迭代 400: 训练集 MSE=71724.5701\n",
      "迭代 500: 训练集 MSE=71724.5701\n",
      "迭代 600: 训练集 MSE=71724.5701\n",
      "迭代 700: 训练集 MSE=71724.5701\n",
      "迭代 800: 训练集 MSE=71724.5701\n",
      "迭代 900: 训练集 MSE=71724.5701\n",
      "迭代 1000: 训练集 MSE=71724.5701\n",
      "最终训练集: MSE=71724.5701, RMSE=267.8144, MAE=218.5905\n",
      "最终测试集: MSE=100576.5645, RMSE=317.1381, MAE=258.8114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义路径\n",
    "fit_results_path = './global_piecewise_linear_fit_results_continuous.csv'  # 分段线性拟合结果\n",
    "smooth_results_path = './smooth_breakpoints_results.csv'  # 平滑化结果\n",
    "output_dir = './predictions'  # 预测结果输出文件夹\n",
    "model_params_dir = './model_params'  # 模型参数保存文件夹\n",
    "epsilon_dir = './epsilon_values'  # 保存迭代过程中的 epsilon 文件夹\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_params_dir, exist_ok=True)\n",
    "os.makedirs(epsilon_dir, exist_ok=True)\n",
    "\n",
    "# 读取分段线性拟合和平滑化结果\n",
    "fit_results_df = pd.read_csv(fit_results_path)\n",
    "smooth_results_df = pd.read_csv(smooth_results_path)\n",
    "\n",
    "# 读取训练和测试数据\n",
    "X_train = pd.read_csv('./X_train.csv').values\n",
    "y_train = pd.read_csv('./Y_train.csv').values.reshape(-1, 1)\n",
    "X_test = pd.read_csv('./X_test.csv').values\n",
    "y_test = pd.read_csv('./Y_test.csv').values.reshape(-1, 1)\n",
    "\n",
    "# 对 X 进行最小-最大归一化到 [-1,1]\n",
    "X_train_min = np.min(X_train, axis=0)\n",
    "X_train_max = np.max(X_train, axis=0)\n",
    "X_train = 2 * (X_train - X_train_min) / (X_train_max - X_train_min) - 1\n",
    "X_test = 2 * (X_test - X_train_min) / (X_train_max - X_train_min) - 1\n",
    "\n",
    "# 在 X_train 和 X_test 最后一列添加全 1 作为偏置项\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "# 超参数\n",
    "delta = 0.0001  # 分段阈值\n",
    "gamma = 0.001  # 学习率衰减率\n",
    "max_iter = 1000  # 最大迭代次数\n",
    "eps = 0  # 避免除零错误\n",
    "\n",
    "\n",
    "# 计算分段梯度\n",
    "def piecewise_gradient(epsilon_i, delta, a_k, breakpoints, smooth_results_df):\n",
    "    \"\"\" 计算样本 i 的梯度 \"\"\"\n",
    "    grad = 0\n",
    "    segment_index = np.digitize([epsilon_i], breakpoints)[0] - 1  # 计算该点在哪个区间\n",
    "    segment_index = int(np.clip(segment_index, 0, len(a_k) - 1))  # 确保是整数索引\n",
    "\n",
    "    # 检查是否在分段点附近\n",
    "    if np.abs(epsilon_i - breakpoints[segment_index]) < delta:\n",
    "        # 查询二次函数的平滑参数\n",
    "        smooth_params = smooth_results_df[smooth_results_df['Breakpoint'] == breakpoints[segment_index]]\n",
    "        if not smooth_params.empty:\n",
    "            a_quad = smooth_params['A'].values[0]\n",
    "            b_quad = smooth_params['B'].values[0]\n",
    "            # 计算二次函数梯度\n",
    "            grad = 2 * a_quad * epsilon_i + b_quad\n",
    "    else:\n",
    "        # 普通分段内，梯度取 a_k[segment_index]\n",
    "        grad = a_k[segment_index]\n",
    "    return grad\n",
    "\n",
    "\n",
    "# 计算误差\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mse, rmse, mae\n",
    "\n",
    "\n",
    "# 训练和预测函数\n",
    "def train_and_predict():\n",
    "    \"\"\" 对整体数据进行训练和预测 \"\"\"\n",
    "    a_k = fit_results_df['Slope'].values  # 斜率 a_k\n",
    "    breakpoints = fit_results_df['Breakpoint_Start'].values  # 分段点\n",
    "\n",
    "    # 初始化参数 w\n",
    "    n, d = X_train.shape\n",
    "    w = np.zeros((d, 1))\n",
    "    eta = 6  # 初始学习率\n",
    "\n",
    "    # 训练过程\n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad_w = np.zeros_like(w)\n",
    "        for i in range(n):  # 遍历所有样本\n",
    "            epsilon_i = (X_train[i] @ w - y_train[i]) / y_train[i]\n",
    "            grad_L_i = piecewise_gradient(epsilon_i, delta, a_k, breakpoints, smooth_results_df)\n",
    "            grad_w += (grad_L_i / y_train[i]) * X_train[i].reshape(-1, 1)\n",
    "        w -= eta * grad_w  # 梯度更新\n",
    "        eta = eta / (1 + gamma * t)  # 学习率衰减\n",
    "\n",
    "        # 每 100 轮输出一次进程信息\n",
    "        if t % 100 == 0:\n",
    "            y_pred_train = X_train @ w\n",
    "            mse_train, _, _ = evaluate(y_train, y_pred_train)\n",
    "            print(f\"迭代 {t}: 训练集 MSE={mse_train:.4f}\")\n",
    "            np.save(os.path.join(model_params_dir, f'w_iter_{t}.npy'), w)\n",
    "\n",
    "    # 训练集和测试集预测\n",
    "    y_pred_train = X_train @ w\n",
    "    y_pred_test = X_test @ w\n",
    "\n",
    "    # 计算最终评价指标\n",
    "    mse_train, rmse_train, mae_train = evaluate(y_train, y_pred_train)\n",
    "    mse_test, rmse_test, mae_test = evaluate(y_test, y_pred_test)\n",
    "    print(f\"最终训练集: MSE={mse_train:.4f}, RMSE={rmse_train:.4f}, MAE={mae_train:.4f}\")\n",
    "    print(f\"最终测试集: MSE={mse_test:.4f}, RMSE={rmse_test:.4f}, MAE={mae_test:.4f}\")\n",
    "\n",
    "    # 保存预测结果\n",
    "    pd.DataFrame({\"y_true\": y_train.flatten(), \"y_pred\": y_pred_train.flatten()}).to_csv(\n",
    "        os.path.join(output_dir, 'predictions_train.csv'), index=False)\n",
    "    pd.DataFrame({\"y_true\": y_test.flatten(), \"y_pred\": y_pred_test.flatten()}).to_csv(\n",
    "        os.path.join(output_dir, '666.csv'), index=False)\n",
    "\n",
    "    # 保存最终模型参数\n",
    "    np.save(os.path.join(model_params_dir, 'w_final.npy'), w)\n",
    "\n",
    "\n",
    "# 训练和预测\n",
    "train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_27240\\2298993246.py:59: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  slopes[seg_idx] * (linear_pred - y_train.iloc[p]) + intercepts[seg_idx] <= t[p] + M * (1 - z[p, seg_idx]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_27240\\2298993246.py\", line 117, in <module>\n",
      "    train_and_predict()\n",
      "  File \"C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_27240\\2298993246.py\", line 80, in train_and_predict\n",
      "    theta_result, beta_result, t_values = train_model(X_train, Y_train, breakpoints, slopes, intercepts)\n",
      "  File \"C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_27240\\2298993246.py\", line 59, in train_model\n",
      "    slopes[seg_idx] * (linear_pred - y_train.iloc[p]) + intercepts[seg_idx] <= t[p] + M * (1 - z[p, seg_idx]),\n",
      "  File \"src\\\\gurobipy\\\\linexpr.pxi\", line 506, in gurobipy.LinExpr.__sub__\n",
      "  File \"src\\\\gurobipy\\\\linexpr.pxi\", line 481, in gurobipy.LinExpr.__add__\n",
      "  File \"src\\\\gurobipy\\\\linexpr.pxi\", line 210, in gurobipy.LinExpr.add\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\ops\\common.py\", line 76, in new_method\n",
      "    return method(self, other)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\arraylike.py\", line 206, in __rmul__\n",
      "    return self._arith_method(other, roperator.rmul)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\series.py\", line 6135, in _arith_method\n",
      "    return base.IndexOpsMixin._arith_method(self, other, op)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\base.py\", line 1384, in _arith_method\n",
      "    return self._construct_result(result, name=res_name)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\series.py\", line 6231, in _construct_result\n",
      "    out = self._constructor(result, index=self.index, dtype=dtype, copy=False)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\series.py\", line 593, in __init__\n",
      "    self.name = name\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\generic.py\", line 6312, in __setattr__\n",
      "    object.__getattribute__(self, name)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\series.py\", line 782, in name\n",
      "    return self._name\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\pandas\\core\\generic.py\", line 6293, in __getattr__\n",
      "    name not in self._internal_names_set\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\inspect.py\", line 1006, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"d:\\Conda\\envs\\NNLLL\\lib\\inspect.py\", line 827, in findsource\n",
      "    raise OSError('source code not available')\n",
      "OSError: source code not available\n"
     ]
    }
   ],
   "source": [
    "#优化法：太慢\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# 读取分段斜率和截距的文件\n",
    "fit_results_df = pd.read_csv('./global_piecewise_linear_fit_results_continuous.csv')\n",
    "\n",
    "# 读取训练数据和测试数据\n",
    "X_train = pd.read_csv('./X_train.csv')\n",
    "Y_train = pd.read_csv('./Y_train.csv')\n",
    "X_test = pd.read_csv('./X_test.csv')\n",
    "Y_test = pd.read_csv('./Y_test.csv')\n",
    "\n",
    "# 获取全局的分段点和斜率和截距\n",
    "breakpoints = fit_results_df['Breakpoint_Start'].tolist() + [fit_results_df['Breakpoint_End'].tolist()[-1]]\n",
    "slopes = fit_results_df['Slope'].tolist()\n",
    "intercepts = fit_results_df['Intercept'].tolist()\n",
    "\n",
    "# 定义函数训练全局的线性回归模型\n",
    "def train_model(X_train, y_train, breakpoints, slopes, intercepts):\n",
    "    # 创建 Gurobi 模型\n",
    "    model = gp.Model('global_linear_regression')\n",
    "    #model.setParam('OutputFlag', 0)  # 关闭输出日志\n",
    "\n",
    "    # 定义决策变量 theta 和 beta\n",
    "    theta = model.addVars(X_train.shape[1], lb=-GRB.INFINITY, vtype=gp.GRB.CONTINUOUS, name='theta')\n",
    "    beta = model.addVar(lb=-GRB.INFINITY, vtype=gp.GRB.CONTINUOUS, name='beta')\n",
    "\n",
    "    # 定义中间变量 t 和分段区间指示变量 z\n",
    "    t = model.addVars(len(y_train), vtype=gp.GRB.CONTINUOUS, name='t')\n",
    "    z = model.addVars(len(y_train), len(breakpoints) - 1, vtype=gp.GRB.BINARY, name='z')\n",
    "\n",
    "    # 设置目标函数：最小化 t 的均值\n",
    "    model.setObjective(gp.quicksum(t[s] for s in range(len(y_train))) / len(y_train), gp.GRB.MINIMIZE)\n",
    "\n",
    "    # 添加约束条件\n",
    "    M = 1e6  # 足够大的常数\n",
    "    epsilon = 1e-6  # 用于处理严格不等式的小常数\n",
    "    for p in range(len(y_train)):\n",
    "        # 计算线性预测值\n",
    "        linear_pred = gp.quicksum(theta[i] * X_train.iloc[p, i] for i in range(X_train.shape[1])) + beta\n",
    "\n",
    "        # 添加分段区间约束\n",
    "        for seg_idx in range(len(breakpoints) - 1):\n",
    "            # 约束 linear_pred >= breakpoints[seg_idx] - M * (1 - z[p, seg_idx])\n",
    "            model.addConstr(\n",
    "                linear_pred >= breakpoints[seg_idx] - M * (1 - z[p, seg_idx]),\n",
    "                name=f'seg_lower_{p}_{seg_idx}'\n",
    "            )\n",
    "            # 约束 linear_pred <= breakpoints[seg_idx + 1] + M * (1 - z[p, seg_idx]) - epsilon\n",
    "            model.addConstr(\n",
    "                linear_pred <= breakpoints[seg_idx + 1] + M * (1 - z[p, seg_idx]) - epsilon,\n",
    "                name=f'seg_upper_{p}_{seg_idx}'\n",
    "            )\n",
    "            # 添加分段线性约束（仅当 z[p, seg_idx] = 1 时激活）\n",
    "            model.addConstr(\n",
    "                slopes[seg_idx] * (linear_pred - y_train.iloc[p]) + intercepts[seg_idx] <= t[p] + M * (1 - z[p, seg_idx]),\n",
    "                name=f'constraint_seg_{p}_{seg_idx}'\n",
    "            )\n",
    "        # 确保 linear_pred 只能属于一个分段区间\n",
    "        model.addConstr(gp.quicksum(z[p, seg_idx] for seg_idx in range(len(breakpoints) - 1)) == 1, name=f'seg_assignment_{p}')\n",
    "\n",
    "    # 优化模型\n",
    "    model.optimize()\n",
    "\n",
    "    # 获取优化后的 theta 和 beta 值\n",
    "    theta_result = np.array([theta[i].x for i in range(X_train.shape[1])])\n",
    "    beta_result = beta.x\n",
    "\n",
    "    # 获取优化后的 t 值\n",
    "    t_values = np.array([t[p].x for p in range(len(y_train))])\n",
    "\n",
    "    return theta_result, beta_result, t_values\n",
    "\n",
    "# 主函数\n",
    "def train_and_predict():\n",
    "    # 训练模型\n",
    "    theta_result, beta_result, t_values = train_model(X_train, Y_train, breakpoints, slopes, intercepts)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"优化后的 theta 值：\", theta_result)\n",
    "    print(\"优化后的 beta 值：\", beta_result)\n",
    "\n",
    "    # 将 theta 和 beta 结果保存到 DataFrame\n",
    "    theta_results_df = pd.DataFrame(theta_result.reshape(1, -1), columns=[f'theta_{i}' for i in range(X_train.shape[1])])\n",
    "    theta_results_df.to_csv('theta_results.csv', index=False)\n",
    "    print(\"theta 值已保存到 theta_results.csv 文件中。\")\n",
    "\n",
    "    beta_results_df = pd.DataFrame([beta_result], columns=['Beta'])\n",
    "    beta_results_df.to_csv('beta_results.csv', index=False)\n",
    "    print(\"beta 值已保存到 beta_results.csv 文件中。\")\n",
    "\n",
    "    # 将 t 值保存到 DataFrame\n",
    "    t_values_df = pd.DataFrame(t_values, columns=['t_values'])\n",
    "    t_values_df.to_csv('t_values.csv', index=False)\n",
    "    print(\"t 值已保存到 t_values.csv 文件中。\")\n",
    "\n",
    "    # 预测 Y_train 和 Y_test 值\n",
    "    def predict_y(X, theta, beta):\n",
    "        return np.dot(X, theta) + beta\n",
    "\n",
    "    y_train_pred = predict_y(X_train, theta_result, beta_result)\n",
    "    y_test_pred = predict_y(X_test, theta_result, beta_result)\n",
    "\n",
    "    # 将预测值保存为 CSV 文件\n",
    "    y_train_pred_df = pd.DataFrame(y_train_pred, columns=['Predicted_Y'])\n",
    "    y_train_pred_df.to_csv('y_train_predictions.csv', index=False)\n",
    "    print(\"Y_train 预测值已保存到 y_train_predictions.csv 文件中。\")\n",
    "\n",
    "    y_test_pred_df = pd.DataFrame(y_test_pred, columns=['Predicted_Y'])\n",
    "    y_test_pred_df.to_csv('y_test_predictions.csv', index=False)\n",
    "    print(\"Y_test 预测值已保存到 y_test_predictions.csv 文件中。\")\n",
    "\n",
    "# 执行主函数\n",
    "train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test 和 Y_test 已按 24 小时分割并保存到 ./test_split 文件夹中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义函数将 X_test 和 Y_test 按 24 小时分割并保存\n",
    "def split_test_data_by_hour(X_test_file, Y_test_file, output_folder, start_hour=7):\n",
    "    # 加载 X_test 和 Y_test 文件\n",
    "    X_test = pd.read_csv(X_test_file)\n",
    "    Y_test = pd.read_csv(Y_test_file)\n",
    "\n",
    "    # 确保输出文件夹存在\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # 按 24 小时分割数据\n",
    "    for hour in range(24):\n",
    "        # 计算实际的小时索引（从 start_hour 开始）\n",
    "        actual_hour = (start_hour - 1 + hour) % 24 + 1\n",
    "\n",
    "        # 提取符合条件的行\n",
    "        extracted_X = X_test.iloc[hour::24, :]\n",
    "        extracted_Y = Y_test.iloc[hour::24, :]\n",
    "\n",
    "        # 保存提取的数据到对应的文件\n",
    "        extracted_X.to_csv(f'{output_folder}/X_test_hour_{actual_hour}.csv', index=False)\n",
    "        extracted_Y.to_csv(f'{output_folder}/Y_test_hour_{actual_hour}.csv', index=False)\n",
    "\n",
    "    print(f\"X_test 和 Y_test 已按 24 小时分割并保存到 {output_folder} 文件夹中。\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 定义 X_test 和 Y_test 文件路径\n",
    "    X_test_file = 'X_test.csv'\n",
    "    Y_test_file = 'Y_test.csv'\n",
    "\n",
    "    # 定义输出文件夹\n",
    "    output_folder = './test_split'\n",
    "\n",
    "    # 调用函数分割数据，指定从 7 点开始\n",
    "    split_test_data_by_hour(X_test_file, Y_test_file, output_folder, start_hour=7)\n",
    "\n",
    "# 执行主函数\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按 trend 列排序后的 Y_test 预测值已保存到 sorted_y_test_predictions.csv 文件中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义函数进行单小时预测\n",
    "def predict_hourly_y(hour, X_test, theta, beta):\n",
    "    # 使用 theta 和 beta 进行线性预测\n",
    "    y_pred = np.dot(X_test, theta) + beta\n",
    "    return y_pred\n",
    "\n",
    "# 定义函数将预测值按 trend 列排序并重新组合\n",
    "def combine_and_sort_predictions(predictions, trend_values):\n",
    "    # 将所有预测值合并为一个数组\n",
    "    all_predictions = np.concatenate(predictions)\n",
    "    # 将所有 trend 值合并为一个数组\n",
    "    all_trends = np.concatenate(trend_values)\n",
    "    # 根据 trend 值排序\n",
    "    sorted_indices = np.argsort(all_trends)\n",
    "    # 按排序后的顺序重新组合预测值\n",
    "    sorted_predictions = all_predictions[sorted_indices]\n",
    "    return sorted_predictions\n",
    "\n",
    "# 主函数\n",
    "def predict_and_combine():\n",
    "    # 加载训练好的 theta 和 beta 值\n",
    "    theta_results_df = pd.read_csv('hourly_theta_results.csv', index_col='Hour')\n",
    "    beta_results_df = pd.read_csv('hourly_beta_results.csv', index_col='Hour')\n",
    "\n",
    "    # 初始化一个列表，用于存储所有小时的预测值\n",
    "    all_predictions = []\n",
    "    # 初始化一个列表，用于存储所有小时的 trend 值\n",
    "    all_trends = []\n",
    "\n",
    "    # 遍历 24 小时\n",
    "    for hour in range(1, 25):\n",
    "        # 加载对应小时的测试数据\n",
    "        X_test_file = f'./test_split/X_test_hour_{hour}.csv'\n",
    "        X_test = pd.read_csv(X_test_file)\n",
    "\n",
    "        # 获取对应小时的 theta 和 beta 值\n",
    "        theta = theta_results_df.loc[hour].values\n",
    "        beta = beta_results_df.loc[hour, 'Beta']\n",
    "\n",
    "        # 进行预测\n",
    "        y_pred = predict_hourly_y(hour, X_test, theta, beta)\n",
    "\n",
    "        # 将预测值添加到列表中\n",
    "        all_predictions.append(y_pred)\n",
    "        # 将 trend 值添加到列表中\n",
    "        all_trends.append(X_test['trend'].values)\n",
    "\n",
    "    # 将预测值按 trend 列排序并重新组合\n",
    "    sorted_predictions = combine_and_sort_predictions(all_predictions, all_trends)\n",
    "\n",
    "    # 将排序后的预测值保存为 CSV 文件\n",
    "    sorted_predictions_df = pd.DataFrame(sorted_predictions, columns=['Predicted_Y'])\n",
    "    sorted_predictions_df.to_csv('sorted_y_test_predictions.csv', index=False)\n",
    "    print(\"按 trend 列排序后的 Y_test 预测值已保存到 sorted_y_test_predictions.csv 文件中。\")\n",
    "\n",
    "# 执行主函数\n",
    "predict_and_combine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\85033\\AppData\\Local\\Temp\\ipykernel_18148\\489176806.py:48: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  y_value = float(y_train.iloc[p])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 11.0.0 build v11.0.0rc2 (win64 - Windows 11+.0 (22631.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i5-13400F, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 10 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 1638 rows, 27861 columns and 26208 nonzeros\n",
      "Model fingerprint: 0x57bbfdb8\n",
      "Model has 3276 quadratic constraints\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  QMatrix range    [2e+02, 1e+07]\n",
      "  QLMatrix range   [1e+00, 2e+06]\n",
      "  Objective range  [6e-04, 6e-04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "  QRHS range       [8e+02, 3e+03]\n",
      "\n",
      "Continuous model is non-convex -- solving as a MIP\n",
      "\n",
      "Presolve time: 0.60s\n",
      "Presolved: 1577394 rows, 420982 columns, 4013100 nonzeros\n",
      "Presolved model has 393120 bilinear constraint(s)\n",
      "Warning: Model contains variables with very large bounds participating\n",
      "         in product terms.\n",
      "         Presolve was not able to compute smaller bounds for these variables.\n",
      "         Consider bounding these variables or reformulating the model.\n",
      "\n",
      "Variable types: 420982 continuous, 0 integer (0 binary)\n",
      "Found heuristic solution: objective 0.0000081\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# 读取分段斜率和截距的文件\n",
    "fit_results_df = pd.read_csv('./global_piecewise_linear_fit_results_continuous.csv')\n",
    "\n",
    "# 读取训练数据和测试数据\n",
    "X_train = pd.read_csv('./X_train.csv')\n",
    "Y_train = pd.read_csv('./Y_train.csv')\n",
    "X_test = pd.read_csv('./X_test.csv')\n",
    "Y_test = pd.read_csv('./Y_test.csv')\n",
    "\n",
    "# 获取全局的分段点、斜率和截距\n",
    "breakpoints = fit_results_df['Breakpoint_Start'].tolist() + [fit_results_df['Breakpoint_End'].tolist()[-1]]\n",
    "slopes = fit_results_df['Slope'].tolist()\n",
    "intercepts = fit_results_df['Intercept'].tolist()\n",
    "num_segments = len(slopes)  # 分段数量\n",
    "\n",
    "def train_model(X_train, y_train, breakpoints, slopes, intercepts):\n",
    "    # 创建 Gurobi 模型\n",
    "    model = gp.Model('global_linear_regression')\n",
    "    \n",
    "    # 定义决策变量 theta 和 beta\n",
    "    theta = model.addVars(X_train.shape[1], lb=-GRB.INFINITY, vtype=GRB.CONTINUOUS, name='theta')\n",
    "    beta = model.addVar(lb=-GRB.INFINITY, vtype=GRB.CONTINUOUS, name='beta')\n",
    "    \n",
    "    # 定义辅助变量 lambda 用于凸组合权重，t 作为误差变量\n",
    "    lambdas = model.addVars(len(y_train), num_segments, lb=0, ub=1, vtype=GRB.CONTINUOUS, name='lambda')\n",
    "    t = model.addVars(len(y_train), vtype=GRB.CONTINUOUS, name='t')\n",
    "\n",
    "    # 设置目标函数：最小化 t 的均值\n",
    "    model.setObjective(gp.quicksum(t[s] for s in range(len(y_train))) / len(y_train), GRB.MINIMIZE)\n",
    "\n",
    "    # 约束：lambda 之和等于 1（确保每个样本在多个分段间的凸组合）\n",
    "    for p in range(len(y_train)):\n",
    "        model.addConstr(gp.quicksum(lambdas[p, s] for s in range(num_segments)) == 1, name=f'lambda_sum_{p}')\n",
    "    \n",
    "    # 约束：拟合误差 t\n",
    "    for p in range(len(y_train)):\n",
    "        linear_pred = gp.quicksum(theta[i] * X_train.iloc[p, i] for i in range(X_train.shape[1])) + beta\n",
    "        piecewise_value = gp.quicksum(\n",
    "            lambdas[p, s] * (slopes[s] * linear_pred + intercepts[s]) for s in range(num_segments)\n",
    "        )\n",
    "    \n",
    "    # 将 y_train.iloc[p] 转换为 float 类型\n",
    "        y_value = float(y_train.iloc[p])\n",
    "    \n",
    "    # 误差约束\n",
    "        model.addConstr(piecewise_value - y_value <= t[p], name=f'err_upper_{p}')\n",
    "        model.addConstr(y_value - piecewise_value <= t[p], name=f'err_lower_{p}')\n",
    "\n",
    "    \n",
    "    # 优化模型\n",
    "    model.optimize()\n",
    "    \n",
    "    # 获取优化后的参数\n",
    "    theta_result = np.array([theta[i].x for i in range(X_train.shape[1])])\n",
    "    beta_result = beta.x\n",
    "    t_values = np.array([t[p].x for p in range(len(y_train))])\n",
    "    \n",
    "    return theta_result, beta_result, t_values\n",
    "\n",
    "# 主函数\n",
    "def train_and_predict():\n",
    "    # 训练模型\n",
    "    theta_result, beta_result, t_values = train_model(X_train, Y_train, breakpoints, slopes, intercepts)\n",
    "    \n",
    "    # 预测函数\n",
    "    def predict_y(X, theta, beta):\n",
    "        return np.dot(X, theta) + beta\n",
    "    \n",
    "    y_train_pred = predict_y(X_train, theta_result, beta_result)\n",
    "    y_test_pred = predict_y(X_test, theta_result, beta_result)\n",
    "\n",
    "    # 保存结果\n",
    "    pd.DataFrame(theta_result.reshape(1, -1), columns=[f'theta_{i}' for i in range(X_train.shape[1])]).to_csv('theta_results.csv', index=False)\n",
    "    pd.DataFrame([beta_result], columns=['Beta']).to_csv('beta_results.csv', index=False)\n",
    "    pd.DataFrame(t_values, columns=['t_values']).to_csv('t_values.csv', index=False)\n",
    "    pd.DataFrame(y_train_pred, columns=['Predicted_Y']).to_csv('y_train_predictions.csv', index=False)\n",
    "    pd.DataFrame(y_test_pred, columns=['Predicted_Y']).to_csv('y_test_predictions.csv', index=False)\n",
    "\n",
    "# 运行\n",
    "train_and_predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNLLL",
   "language": "python",
   "name": "nnlll"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
